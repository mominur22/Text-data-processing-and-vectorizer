{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb210ba",
   "metadata": {},
   "source": [
    "# Stemming NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "550e298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "beb9c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "376941b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efa767be",
   "metadata": {},
   "outputs": [],
   "source": [
    "word=['change','changing','changes','changed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35def62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['change', 'changing', 'changes', 'changed']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d636b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "131b942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e46d02bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chang\n",
      "chang\n",
      "chang\n",
      "chang\n"
     ]
    }
   ],
   "source": [
    "for w in word:\n",
    "    print(p.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d2cb167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change chang\n",
      "changing chang\n",
      "changes chang\n",
      "changed chang\n"
     ]
    }
   ],
   "source": [
    "for w in word:\n",
    "    print(w,p.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f38ee07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen='The constant flux of life necessitates embraching change,whether its adapting to the changes around us or actively changing ourselves to meet new challenges. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c118d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The constant flux of life necessitates embraching change,whether its adapting to the changes around us or actively changing ourselves to meet new challenges. '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f13043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3849299",
   "metadata": {},
   "outputs": [],
   "source": [
    "token=word_tokenize(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9259fdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'constant',\n",
       " 'flux',\n",
       " 'of',\n",
       " 'life',\n",
       " 'necessitates',\n",
       " 'embraching',\n",
       " 'change',\n",
       " ',',\n",
       " 'whether',\n",
       " 'its',\n",
       " 'adapting',\n",
       " 'to',\n",
       " 'the',\n",
       " 'changes',\n",
       " 'around',\n",
       " 'us',\n",
       " 'or',\n",
       " 'actively',\n",
       " 'changing',\n",
       " 'ourselves',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'new',\n",
       " 'challenges',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2190c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sen.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18d39ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "constant\n",
      "flux\n",
      "of\n",
      "life\n",
      "necessit\n",
      "embrach\n",
      "chang\n",
      ",\n",
      "whether\n",
      "it\n",
      "adapt\n",
      "to\n",
      "the\n",
      "chang\n",
      "around\n",
      "us\n",
      "or\n",
      "activ\n",
      "chang\n",
      "ourselv\n",
      "to\n",
      "meet\n",
      "new\n",
      "challeng\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for w in token:\n",
    "    print(p.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2bd356",
   "metadata": {},
   "source": [
    "# Lematization in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0db940aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87be1f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "le=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7938a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'constant',\n",
       " 'flux',\n",
       " 'of',\n",
       " 'life',\n",
       " 'necessitates',\n",
       " 'embraching',\n",
       " 'change',\n",
       " ',',\n",
       " 'whether',\n",
       " 'its',\n",
       " 'adapting',\n",
       " 'to',\n",
       " 'the',\n",
       " 'changes',\n",
       " 'around',\n",
       " 'us',\n",
       " 'or',\n",
       " 'actively',\n",
       " 'changing',\n",
       " 'ourselves',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'new',\n",
       " 'challenges',\n",
       " '.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22452b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1daa5330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "constant\n",
      "flux\n",
      "of\n",
      "life\n",
      "necessitates\n",
      "embraching\n",
      "change\n",
      ",\n",
      "whether\n",
      "it\n",
      "adapting\n",
      "to\n",
      "the\n",
      "change\n",
      "around\n",
      "u\n",
      "or\n",
      "actively\n",
      "changing\n",
      "ourselves\n",
      "to\n",
      "meet\n",
      "new\n",
      "challenge\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for w in token:\n",
    "    print(le.lemmatize(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe58e35",
   "metadata": {},
   "source": [
    "# Tokenization in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd2ba097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'from', 'bangladesh.i', 'am', 'learning', 'Nlp.it', 'is', 'fascinating']\n",
      "['i am from bangladesh.i am learning Nlp.it is fascinating']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "sentence= 'i am from bangladesh.i am learning Nlp.it is fascinating'\n",
    "word_tokens=word_tokenize(sentence)\n",
    "sentence_tokens=sent_tokenize(sentence)\n",
    "print(word_tokens)\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37ecf31",
   "metadata": {},
   "source": [
    "# spaCY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f69f7c",
   "metadata": {},
   "source": [
    "spaCy is fast and efficient at runtime, making it a good choice for building production-level NLP applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0f2d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "# python -m spacy download en_core_web_sm   < installed in conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be727c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'from', 'aiQuest', 'Intelligence', '.', 'I', 'am', 'learning', 'NLP', '.', 'It', 'is', 'fascinating', '!']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spc = spacy.load('en_core_web_sm')  # Load the English language model\n",
    "\n",
    "sentence = \"I'm from aiQuest Intelligence. I am learning NLP. It is fascinating!\"\n",
    "doc = spc(sentence)\n",
    "\n",
    "word_tokens = [token.text for token in doc]\n",
    "\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64499128",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb4b514",
   "metadata": {},
   "source": [
    "Transformers is a library built by Hugging Face that provides state-of-the-art pre-trained models for NLP. It offers various functionalities, including tokenization. To install Transformers, run pip install transformers. Here's an example of tokenization using Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40ae236d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 'm', 'from', 'ai', '##quest', 'intelligence', '.', 'i', 'am', 'learning', 'nl', '##p', '.', 'it', 'is', 'fascinating', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sentence = \"I'm from aiQuest Intelligence. I am learning NLP. It is fascinating!\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6507309e",
   "metadata": {},
   "source": [
    "# Named Entity Tokenization using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af1f39",
   "metadata": {},
   "source": [
    "To perform named entity tokenization using NLTK (Natural Language Toolkit), you can utilize the named entity recognition (NER) functionality provided by NLTK. Here's an example of how to extract named entity tokens from a sentence using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ac938a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aiQuest Intelligence', 'NLP', 'Hasan', 'Joe']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "sentence = \"I'm from aiQuest Intelligence. I am learning NLP. It is fascinating!, Hasan vai, my name is Joe\"\n",
    "\n",
    "tokens = word_tokenize(sentence) # Tokenize the sentence into words\n",
    "\n",
    "pos_tags = pos_tag(tokens) # Perform part-of-speech tagging\n",
    "\n",
    "ner_tags = ne_chunk(pos_tags) # Perform named entity recognition\n",
    "\n",
    "named_entity_tokens = []\n",
    "\n",
    "for chunk in ner_tags:\n",
    "    if hasattr(chunk, 'label'):\n",
    "        named_entity_tokens.append(' '.join(c[0] for c in chunk))\n",
    "\n",
    "print(named_entity_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3bc2f437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('maxent_ne_chunker')  # Download the required resource (NER models)\n",
    "nltk.download('words')  # Download the required resource (word corpus) \n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a120bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"Shakil Lives in Germany\"\n",
    "tokens = word_tokenize(sentence2)\n",
    "pos_tags = pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8057ab79",
   "metadata": {},
   "source": [
    "# Text Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dcdfc6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "56c24944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey, I love Bangladesh;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good afternoon, I am happy!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I live in Germany</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice to meet you man-</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You won an iPhone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text  class\n",
       "0      Hey, I love Bangladesh;      1\n",
       "1  Good afternoon, I am happy!      1\n",
       "2            I live in Germany      1\n",
       "3        Nice to meet you man-      1\n",
       "4            You won an iPhone      0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a473058",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03061dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "en_stopwords = set(stopwords.words('english')) \n",
    "en_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62bd15d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aed02e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2bb99dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "61fd4f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e0388b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 54]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = [1,2,3,4,54]\n",
    "[l for l in li ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "05c8e26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 54]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[l for l in li if l%2==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "03e26ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey, I love Bangladesh;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good afternoon, I am happy!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I live in Germany</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice to meet you man-</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You won an iPhone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text  class\n",
       "0      Hey, I love Bangladesh;      1\n",
       "1  Good afternoon, I am happy!      1\n",
       "2            I live in Germany      1\n",
       "3        Nice to meet you man-      1\n",
       "4            You won an iPhone      0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4305196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text): \n",
    "    \n",
    "    remove_punc = [char for char in text if char not in string.punctuation] # Remove punctuation\n",
    "    clean_words = ''.join(remove_punc) # char joining\n",
    "    \n",
    "    #Remove stopwords\n",
    "    text = ([word for word in clean_words.split() if word.lower() not in en_stopwords]) # stopword = stopwords.words('english')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1be5fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(preprocess_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d34a4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [Hey, love, Bangladesh]\n",
       "1    [Good, afternoon, happy]\n",
       "2             [live, Germany]\n",
       "3           [Nice, meet, man]\n",
       "4                    [iPhone]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73e41296",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in text])\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a0598cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey love Bangladesh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good afternoon happy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>live Germany</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice meet man</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   text  class\n",
       "0   Hey love Bangladesh      1\n",
       "1  Good afternoon happy      1\n",
       "2          live Germany      1\n",
       "3         Nice meet man      1\n",
       "4                iPhone      0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(lemmatize_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9958b2a",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "05ac52cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "da76c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b2dccec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x12 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_x = cv.fit_transform(df['text'])\n",
    "cv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "498aacf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "99beb54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   10  11\n",
       "0   0   1   0   0   0   1   0   0   1   0   0   0\n",
       "1   1   0   0   1   1   0   0   0   0   0   0   0\n",
       "2   0   0   1   0   0   0   0   1   0   0   0   0\n",
       "3   0   0   0   0   0   0   0   0   0   1   1   1\n",
       "4   0   0   0   0   0   0   1   0   0   0   0   0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df = pd.DataFrame(cv_x.toarray())\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6f6c1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['afternoon', 'bangladesh', 'germany', 'good', 'happy', 'hey',\n",
       "       'iphone', 'live', 'love', 'man', 'meet', 'nice'], dtype=object)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "429ac60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df = pd.DataFrame(cv_x.toarray(), index=df['text'], columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c50bb6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>afternoon</th>\n",
       "      <th>bangladesh</th>\n",
       "      <th>germany</th>\n",
       "      <th>good</th>\n",
       "      <th>happy</th>\n",
       "      <th>hey</th>\n",
       "      <th>iphone</th>\n",
       "      <th>live</th>\n",
       "      <th>love</th>\n",
       "      <th>man</th>\n",
       "      <th>meet</th>\n",
       "      <th>nice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hey love Bangladesh</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Good afternoon happy</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>live Germany</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nice meet man</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iPhone</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      afternoon  bangladesh  germany  good  happy  hey  \\\n",
       "text                                                                     \n",
       "Hey love Bangladesh           0           1        0     0      0    1   \n",
       "Good afternoon happy          1           0        0     1      1    0   \n",
       "live Germany                  0           0        1     0      0    0   \n",
       "Nice meet man                 0           0        0     0      0    0   \n",
       "iPhone                        0           0        0     0      0    0   \n",
       "\n",
       "                      iphone  live  love  man  meet  nice  \n",
       "text                                                       \n",
       "Hey love Bangladesh        0     0     1    0     0     0  \n",
       "Good afternoon happy       0     0     0    0     0     0  \n",
       "live Germany               0     1     0    0     0     0  \n",
       "Nice meet man              0     0     0    1     1     1  \n",
       "iPhone                     1     0     0    0     0     0  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bfc9fe",
   "metadata": {},
   "source": [
    "# TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e75d88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ae780dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_z = tf.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d2e52b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x12 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "80572045",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df = pd.DataFrame(tf_z.toarray(), index=df['text'], columns=tf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "897b301d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>afternoon</th>\n",
       "      <th>bangladesh</th>\n",
       "      <th>germany</th>\n",
       "      <th>good</th>\n",
       "      <th>happy</th>\n",
       "      <th>hey</th>\n",
       "      <th>iphone</th>\n",
       "      <th>live</th>\n",
       "      <th>love</th>\n",
       "      <th>man</th>\n",
       "      <th>meet</th>\n",
       "      <th>nice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hey love Bangladesh</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Good afternoon happy</th>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>live Germany</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nice meet man</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iPhone</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      afternoon  bangladesh   germany     good    happy  \\\n",
       "text                                                                      \n",
       "Hey love Bangladesh     0.00000     0.57735  0.000000  0.00000  0.00000   \n",
       "Good afternoon happy    0.57735     0.00000  0.000000  0.57735  0.57735   \n",
       "live Germany            0.00000     0.00000  0.707107  0.00000  0.00000   \n",
       "Nice meet man           0.00000     0.00000  0.000000  0.00000  0.00000   \n",
       "iPhone                  0.00000     0.00000  0.000000  0.00000  0.00000   \n",
       "\n",
       "                          hey  iphone      live     love      man     meet  \\\n",
       "text                                                                         \n",
       "Hey love Bangladesh   0.57735     0.0  0.000000  0.57735  0.00000  0.00000   \n",
       "Good afternoon happy  0.00000     0.0  0.000000  0.00000  0.00000  0.00000   \n",
       "live Germany          0.00000     0.0  0.707107  0.00000  0.00000  0.00000   \n",
       "Nice meet man         0.00000     0.0  0.000000  0.00000  0.57735  0.57735   \n",
       "iPhone                0.00000     1.0  0.000000  0.00000  0.00000  0.00000   \n",
       "\n",
       "                         nice  \n",
       "text                           \n",
       "Hey love Bangladesh   0.00000  \n",
       "Good afternoon happy  0.00000  \n",
       "live Germany          0.00000  \n",
       "Nice meet man         0.57735  \n",
       "iPhone                0.00000  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50a1d2d",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6dfdbd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "81bcf4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f17defca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hey', 'love', 'Bangladesh'],\n",
       " ['Good', 'afternoon', 'happy'],\n",
       " ['live', 'Germany'],\n",
       " ['Nice', 'meet', 'man'],\n",
       " ['iPhone']]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vector = [nltk.word_tokenize(test) for test in df['text']]\n",
    "text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c0bf9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(text_vector, min_count=1) #shift+tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0dbb3a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x2477481d6d0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "de21d8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meet', 0.14595060050487518),\n",
       " ('love', 0.05048206076025963),\n",
       " ('Nice', 0.04157734662294388),\n",
       " ('Germany', 0.03476495295763016),\n",
       " ('live', 0.019152265042066574),\n",
       " ('iPhone', 0.016134697943925858),\n",
       " ('Good', 0.008826171047985554),\n",
       " ('afternoon', 0.004842488560825586),\n",
       " ('Bangladesh', 0.0019510717829689384),\n",
       " ('Hey', -0.08382602781057358)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a47a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
